"""
é‡å­å¹¶è¡Œå¤„ç†å™¨ - é›†æˆåˆ°Tavilyé¡¹ç›®
åŸºäºwuyueé‡å­æ¡†æ¶å®ç°å¤šå…¬å¸å¹¶è¡Œåˆ†æ
"""

import asyncio
import logging
import os
import json
import math
import numpy as np
from datetime import datetime
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor

# å¯¼å…¥wuyueé‡å­æ¡†æ¶
from wuyue_machine_learning.encoding import AmplitudeEncoding, AngleEncoding
from wuyue.register.quantumregister import QuantumRegister
from wuyue.register.classicalregister import ClassicalRegister
from wuyue.circuit.circuit import QuantumCircuit
from wuyue.element.gate import H, RX, RY, RZ, CNOT, CZ, MEASURE
from wuyue.backend import Backend

# å¯¼å…¥Tavilyç»„ä»¶
from ..graph import Graph

logger = logging.getLogger(__name__)


class QuantumParallelProcessor:
    """
    é‡å­å¹¶è¡Œå¤„ç†å™¨
    ç»“åˆTavilyçš„é«˜è´¨é‡æ•°æ®æ”¶é›†å’Œé‡å­å¹¶è¡Œè®¡ç®—èƒ½åŠ›
    """
    
    def __init__(self, max_companies: int = 8, n_layers: int = 3, shots: int = 1000):
        """
        åˆå§‹åŒ–é‡å­å¹¶è¡Œå¤„ç†å™¨

        Args:
            max_companies: æ”¯æŒçš„æœ€å¤§å…¬å¸æ•°é‡
            n_layers: é‡å­çº¿è·¯å±‚æ•°
            shots: é‡å­æµ‹é‡æ¬¡æ•°
        """
        self.max_companies = max_companies
        self.n_layers = n_layers
        self.shots = shots

        # é‡å­å‚æ•°
        self.n_qubits = math.ceil(math.log2(max_companies))  # å…¬å¸ç´¢å¼•é‡å­æ¯”ç‰¹
        self.feature_qubits = 4  # ç‰¹å¾é‡å­æ¯”ç‰¹
        self.total_qubits = self.n_qubits + self.feature_qubits

        # é‡å­åç«¯ - å»¶è¿Ÿåˆå§‹åŒ–
        self.backend = None
        self._initialize_backend()

        # çŸ¥è¯†åº“è®¾ç½®
        self._setup_knowledge_base()

        logger.info(f"ğŸ”¬ é‡å­å¹¶è¡Œå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ: {self.total_qubits}é‡å­æ¯”ç‰¹, {n_layers}å±‚, {shots}æ¬¡æµ‹é‡")

    def _initialize_backend(self):
        """åˆå§‹åŒ–é‡å­åç«¯"""
        try:
            self.backend = Backend.get_device()
            logger.debug("é‡å­åç«¯åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"é‡å­åç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            self.backend = None
    
    def _setup_knowledge_base(self):
        """è®¾ç½®çŸ¥è¯†åº“ç›®å½•"""
        self.knowledge_base_dir = "knowledge_base"
        self.company_reports_dir = os.path.join(self.knowledge_base_dir, "company_reports")
        self.quantum_metadata_dir = os.path.join(self.knowledge_base_dir, "quantum_metadata")
        self.batch_results_dir = os.path.join(self.knowledge_base_dir, "batch_results")
        
        for directory in [self.knowledge_base_dir, self.company_reports_dir, 
                         self.quantum_metadata_dir, self.batch_results_dir]:
            os.makedirs(directory, exist_ok=True)
    
    async def process_companies_quantum_parallel(self, companies: List[Dict[str, str]], 
                                                websocket_manager=None, job_id=None) -> Dict[str, Any]:
        """é‡å­å¹¶è¡Œå¤„ç†å¤šå®¶å…¬å¸"""
        # ä¿å­˜åŸå§‹å…¬å¸æ•°æ®ä¾›åç»­ä½¿ç”¨
        self.original_companies = companies
        
        logger.info(f"ğŸš€ å¼€å§‹é‡å­å¹¶è¡Œåˆ†æ {len(companies)} å®¶å…¬å¸...")
        
        if websocket_manager and job_id:
            await websocket_manager.send_status_update(
                job_id, 
                status="processing", 
                message=f"ğŸ”¬ Starting quantum parallel analysis of {len(companies)} companies"
            )
        
        try:
            # ç¬¬ä¸€é˜¶æ®µï¼šä½¿ç”¨Tavilyå¹¶è¡Œæ”¶é›†é«˜è´¨é‡æ•°æ®
            logger.info("ğŸ“Š é˜¶æ®µ1: ä½¿ç”¨Tavilyæ”¶é›†å…¬å¸æ•°æ®...")
            tavily_data = await self._collect_tavily_data(companies, websocket_manager, job_id)
            
            # ç¬¬äºŒé˜¶æ®µï¼šé‡å­ç¼–ç å’Œå¹¶è¡Œå¤„ç†
            logger.info("âš¡ é˜¶æ®µ2: é‡å­ç¼–ç å’Œå¹¶è¡Œè®¡ç®—...")
            quantum_results = await self._quantum_process(tavily_data, websocket_manager, job_id)
            
            # ç¬¬ä¸‰é˜¶æ®µï¼šèåˆåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ
            logger.info("ğŸ§  é˜¶æ®µ3: èåˆåˆ†æå’ŒæŠ¥å‘Šç”Ÿæˆ...")
            final_reports = await self._generate_enhanced_reports(tavily_data, quantum_results, websocket_manager, job_id)
            
            # ç¬¬å››é˜¶æ®µï¼šä¿å­˜åˆ°çŸ¥è¯†åº“
            logger.info("ğŸ’¾ é˜¶æ®µ4: ä¿å­˜åˆ°çŸ¥è¯†åº“...")
            batch_summary = await self._save_to_knowledge_base(final_reports, companies)
            
            result = {
                "successful_reports": final_reports,
                "failed_companies": [],
                "batch_summary": batch_summary,
                "quantum_metadata": {
                    "total_qubits": self.total_qubits,
                    "quantum_layers": self.n_layers,
                    "measurement_shots": self.shots,
                    "quantum_advantage_enabled": True
                }
            }
            
            if websocket_manager and job_id:
                await websocket_manager.send_status_update(
                    job_id,
                    status="completed",
                    message=f"ğŸ‰ Quantum parallel analysis completed for {len(companies)} companies",
                    result=result
                )
            
            logger.info("âœ… é‡å­å¹¶è¡Œåˆ†æå®Œæˆï¼")
            return result
            
        except Exception as e:
            logger.error(f"âŒ é‡å­å¹¶è¡Œåˆ†æå¤±è´¥: {e}")
            if websocket_manager and job_id:
                await websocket_manager.send_status_update(
                    job_id, status="error", message=f"Quantum analysis failed: {str(e)}"
                )
            raise e
    
    async def _collect_tavily_data(self, companies_data: List[Dict[str, str]], 
                                 websocket_manager, job_id) -> Dict[str, Any]:
        """ä½¿ç”¨Tavilyå¹¶è¡Œæ”¶é›†å…¬å¸æ•°æ®"""
        if websocket_manager and job_id:
            await websocket_manager.send_status_update(
                job_id, status="processing", 
                message="ğŸ“Š Collecting high-quality data using Tavily..."
            )
        
        # å¹¶è¡Œæ‰§è¡ŒTavilyåˆ†æ
        tasks = []
        for i, company_data in enumerate(companies_data):
            task = self._run_tavily_analysis(company_data, i+1, len(companies_data))
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        tavily_data = {}
        for i, result in enumerate(results):
            company_name = companies_data[i]["name"]
            if isinstance(result, Exception):
                logger.error(f"âŒ Tavilyåˆ†æå¤±è´¥ {company_name}: {result}")
                # åˆ›å»ºé»˜è®¤æ•°æ®
                tavily_data[company_name] = {
                    "report": f"Analysis failed for {company_name}",
                    "company_data": {},
                    "financial_data": {},
                    "error": str(result)
                }
            else:
                tavily_data[company_name] = result
        
        return tavily_data
    
    async def _run_tavily_analysis(self, company_data: Dict[str, str], 
                                 company_index: int, total_companies: int) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå…¬å¸çš„Tavilyåˆ†æ"""
        company_name = company_data["name"]
        logger.info(f"ğŸ” åˆ†æ {company_name} ({company_index}/{total_companies})")
        
        # åˆ›å»ºTavily Graphå®ä¾‹
        graph = Graph(
            company=company_name,
            url=company_data.get("company_url", ""),
            industry=company_data.get("industry", ""),
            hq_location=company_data.get("hq_location", ""),
            websocket_manager=None,  # é¿å…WebSocketå†²çª
            job_id=None
        )
        
        # æ‰§è¡ŒTavilyåˆ†æ
        state = {}
        async for s in graph.run(thread={}):
            state.update(s)
        
        # æå–å…³é”®æ•°æ®
        report_content = state.get('report') or (state.get('editor') or {}).get('report', '')
        
        return {
            "company_name": company_name,
            "report": report_content,
            "company_data": state.get('company_data', {}),
            "financial_data": state.get('financial_data', {}),
            "industry_data": state.get('industry_data', {}),
            "news_data": state.get('news_data', {}),
            "references": state.get('references', []),
            "full_state": state
        }
    
    async def _quantum_process(self, tavily_data: Dict[str, Any],
                             websocket_manager, job_id) -> Dict[str, Any]:
        """é‡å­ç¼–ç å’Œå¹¶è¡Œå¤„ç† - ä½¿ç”¨single_agentçš„æ­£ç¡®æ–¹å¼"""
        if websocket_manager and job_id:
            await websocket_manager.send_status_update(
                job_id, status="processing",
                message="âš¡ Quantum encoding and parallel processing..."
            )

        # éªŒè¯Tavilyæ•°æ®è´¨é‡
        valid_companies = 0
        for company_name, data in tavily_data.items():
            report = data.get('report', '')
            if report and len(report.strip()) > 50:  # è‡³å°‘æœ‰50ä¸ªå­—ç¬¦çš„æœ‰æ•ˆæŠ¥å‘Š
                valid_companies += 1

        if valid_companies == 0:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„Tavilyæ•°æ®ï¼Œé‡å­åˆ†æå°†åŸºäºé»˜è®¤å‚æ•°")
            if websocket_manager and job_id:
                await websocket_manager.send_status_update(
                    job_id, status="processing",
                    message="âš ï¸ Limited data available, using quantum analysis with default parameters..."
                )

        # è½¬æ¢ä¸ºsingle_agentæ ¼å¼çš„å…¬å¸æ•°æ®
        companies_data = []
        for company_name, data in tavily_data.items():
            # ä»Tavilyæ•°æ®ä¸­æå–å› å­ä¿¡æ¯
            factors = self._extract_factors_from_tavily_data(data)
            companies_data.append({
                "name": company_name,
                "factors": factors,
                "tavily_data": data
            })

        # ä½¿ç”¨single_agentçš„é‡å­ç¼–ç æ–¹å¼ - ä¸€ä¸ªé‡å­çº¿è·¯å¤„ç†æ‰€æœ‰å…¬å¸
        encoded_qc = self._encode_all_companies_to_single_circuit(companies_data)

        # æ„å»ºåˆ†æçº¿è·¯ï¼ˆåŸºäºsingle_agentçš„æ–¹å¼ï¼‰
        analysis_qc = self._create_analysis_circuit(encoded_qc)

        # æ‰§è¡Œé‡å­è®¡ç®— - åªè°ƒç”¨ä¸€æ¬¡ï¼
        measurement_results = self._execute_single_quantum_circuit(analysis_qc)

        # åˆ†æé‡å­ç»“æœ
        quantum_analysis = self._analyze_quantum_results(measurement_results, companies_data)

        return quantum_analysis

    def _extract_factors_from_tavily_data(self, tavily_data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ä»Tavilyæ•°æ®ä¸­æå–å› å­ä¿¡æ¯ï¼Œè½¬æ¢ä¸ºsingle_agentæ ¼å¼"""
        factors = []

        # ä»æŠ¥å‘Šé•¿åº¦æå–ä¿¡æ¯ä¸°å¯Œåº¦å› å­
        report_length = len(tavily_data.get('report', ''))
        factors.append({
            "name": "ä¿¡æ¯ä¸°å¯Œåº¦",
            "value": min(report_length / 1000.0, 10.0),  # æ ‡å‡†åŒ–åˆ°0-10
            "weight": 0.2
        })

        # ä»æ•°æ®æºæ•°é‡æå–å¯ä¿¡åº¦å› å­
        references_count = len(tavily_data.get('references', []))
        factors.append({
            "name": "æ•°æ®æºå¯ä¿¡åº¦",
            "value": min(references_count / 2.0, 10.0),  # æ ‡å‡†åŒ–åˆ°0-10
            "weight": 0.25
        })

        # ä»è´¢åŠ¡æ•°æ®æå–è´¢åŠ¡å¥åº·åº¦å› å­
        financial_data = tavily_data.get('financial_data', {})
        financial_score = len(str(financial_data)) / 100.0  # ç®€å•çš„è´¢åŠ¡æ•°æ®ä¸°å¯Œåº¦
        factors.append({
            "name": "è´¢åŠ¡å¥åº·åº¦",
            "value": min(financial_score, 10.0),
            "weight": 0.3
        })

        # ä»æ–°é—»æ•°æ®æå–å¸‚åœºæ´»è·ƒåº¦å› å­
        news_data = tavily_data.get('news_data', {})
        news_activity = len(str(news_data)) / 100.0
        factors.append({
            "name": "å¸‚åœºæ´»è·ƒåº¦",
            "value": min(news_activity, 10.0),
            "weight": 0.25
        })

        return factors

    def _encode_all_companies_to_single_circuit(self, companies_data: List[Dict[str, Any]]) -> QuantumCircuit:
        """
        å°†æ‰€æœ‰å…¬å¸ç¼–ç åˆ°å•ä¸ªé‡å­çº¿è·¯ä¸­ - single_agentçš„æ­£ç¡®æ–¹å¼
        è¿™æ˜¯çœŸæ­£çš„é‡å­å¹¶è¡Œï¼šä¸€ä¸ªé‡å­çº¿è·¯åŒæ—¶å¤„ç†æ‰€æœ‰å…¬å¸
        """
        n_companies = len(companies_data)
        if n_companies > self.max_companies:
            raise ValueError(f"å…¬å¸æ•°é‡ {n_companies} è¶…è¿‡æœ€å¤§æ”¯æŒæ•°é‡ {self.max_companies}")

        # åˆ›å»ºé‡å­å¯„å­˜å™¨
        qreg = QuantumRegister(self.total_qubits)
        creg = ClassicalRegister(self.total_qubits)
        qc = QuantumCircuit(qreg, creg)

        # 1. åˆ›å»ºå…¬å¸ç´¢å¼•çš„å åŠ æ€ - å…³é”®ï¼šæ‰€æœ‰å…¬å¸åŒæ—¶å­˜åœ¨ï¼
        for i in range(self.n_qubits):
            qc.add(H, qreg[i])  # |00âŸ© + |01âŸ© + |10âŸ© + |11âŸ©

        # 2. ä¸ºæ¯ä¸ªå…¬å¸ç¼–ç ç‰¹å¾æ•°æ®åˆ°åŒä¸€ä¸ªé‡å­ç³»ç»Ÿ
        for company_idx, company_data in enumerate(companies_data):
            self._encode_single_company_to_circuit(qc, qreg, company_idx, company_data)

        logger.info(f"âœ… æˆåŠŸå°† {n_companies} å®¶å…¬å¸ç¼–ç åˆ°å•ä¸ªé‡å­çº¿è·¯ä¸­")
        return qc

    def _encode_single_company_to_circuit(self, qc: QuantumCircuit, qreg: QuantumRegister,
                                        company_idx: int, company_data: Dict[str, Any]):
        """
        å°†å•ä¸ªå…¬å¸çš„æ•°æ®ç¼–ç åˆ°é‡å­çº¿è·¯ä¸­ - åŸºäºsingle_agentçš„æ–¹æ³•
        """
        factors = company_data.get('factors', [])

        # æå–å› å­ç‰¹å¾
        features = self._extract_features_from_factors(factors)

        # ä½¿ç”¨è§’åº¦ç¼–ç å°†ç‰¹å¾ç¼–ç åˆ°ç‰¹å¾é‡å­æ¯”ç‰¹ä¸Š
        for feature_idx, feature_value in enumerate(features[:self.feature_qubits]):
            target_qubit = self.n_qubits + feature_idx

            # åˆ›å»ºå—æ§æ—‹è½¬é—¨ï¼Œåªæœ‰å½“å…¬å¸ç´¢å¼•åŒ¹é…æ—¶æ‰åº”ç”¨
            control_qubits = self._get_control_qubits_for_company(company_idx)

            # åº”ç”¨å—æ§RYé—¨ç¼–ç ç‰¹å¾å€¼
            self._apply_controlled_rotation(qc, qreg, control_qubits, target_qubit, feature_value)

    def _extract_features_from_factors(self, factors: List[Dict[str, Any]]) -> List[float]:
        """
        ä»å› å­æ•°æ®ä¸­æå–ç‰¹å¾å‘é‡ - åŸºäºsingle_agentçš„æ–¹æ³•
        """
        features = []

        # æå–ä¸»è¦ç‰¹å¾
        for factor in factors:
            value = factor.get('value', 0.0)
            weight = factor.get('weight', 0.0)

            # ç‰¹å¾å·¥ç¨‹ï¼šç»“åˆå€¼å’Œæƒé‡
            weighted_value = value * weight
            features.append(weighted_value)

        # å¡«å……åˆ°å›ºå®šé•¿åº¦
        while len(features) < self.feature_qubits:
            features.append(0.0)

        # æ ‡å‡†åŒ–åˆ° [0, 2Ï€] èŒƒå›´ï¼ˆé€‚åˆè§’åº¦ç¼–ç ï¼‰
        features = np.array(features[:self.feature_qubits])
        if np.max(np.abs(features)) > 0:
            features = (features - np.min(features)) / (np.max(features) - np.min(features)) * 2 * np.pi

        return features.tolist()

    def _get_control_qubits_for_company(self, company_idx: int) -> List[int]:
        """
        è·å–å…¬å¸ç´¢å¼•å¯¹åº”çš„æ§åˆ¶é‡å­æ¯”ç‰¹ - åŸºäºsingle_agentçš„æ–¹æ³•
        """
        binary_repr = format(company_idx, f'0{self.n_qubits}b')
        control_qubits = []

        for i, bit in enumerate(binary_repr):
            if bit == '1':
                control_qubits.append(i)

        return control_qubits

    def _apply_controlled_rotation(self, qc: QuantumCircuit, qreg: QuantumRegister,
                                 control_qubits: List[int], target_qubit: int, angle: float):
        """
        åº”ç”¨å—æ§æ—‹è½¬é—¨ - åŸºäºsingle_agentçš„æ–¹æ³•
        """
        if len(control_qubits) == 0:
            # æ— æ§åˆ¶ä½ï¼Œç›´æ¥åº”ç”¨æ—‹è½¬é—¨
            qc.add(RY, qreg[target_qubit], paras=[angle])
        elif len(control_qubits) == 1:
            # å•æ§åˆ¶ä½
            qc.add(RY, qreg[target_qubit], qreg[control_qubits[0]], paras=[angle])
        else:
            # å¤šæ§åˆ¶ä½ï¼Œç®€åŒ–å¤„ç†
            for ctrl in control_qubits:
                qc.add(RY, qreg[target_qubit], qreg[ctrl], paras=[angle / len(control_qubits)])

    def _create_analysis_circuit(self, encoded_qc: QuantumCircuit) -> QuantumCircuit:
        """
        åˆ›å»ºåˆ†æé‡å­çº¿è·¯ - åŸºäºsingle_agentçš„æ–¹æ³•
        """
        qreg = encoded_qc.qreg

        # æ·»åŠ å˜åˆ†åˆ†æå±‚
        for layer in range(self.n_layers):
            # æ¯ä¸€å±‚åº”ç”¨å‚æ•°åŒ–æ—‹è½¬é—¨
            for qubit in range(len(qreg)):
                # åº”ç”¨RX, RY, RZæ—‹è½¬é—¨
                angle_x = np.random.uniform(0, 2*np.pi)
                angle_y = np.random.uniform(0, 2*np.pi)
                angle_z = np.random.uniform(0, 2*np.pi)

                encoded_qc.add(RX, qreg[qubit], paras=[angle_x])
                encoded_qc.add(RY, qreg[qubit], paras=[angle_y])
                encoded_qc.add(RZ, qreg[qubit], paras=[angle_z])

            # æ·»åŠ çº ç¼ é—¨
            if layer < self.n_layers - 1:  # æœ€åä¸€å±‚ä¸æ·»åŠ çº ç¼ 
                self._add_entanglement_layer(encoded_qc, qreg)

        # æ·»åŠ æœ€ç»ˆçš„ç‰¹å¾æå–å±‚
        for i in range(len(qreg)):
            encoded_qc.add(H, qreg[i])
            angle = np.pi / 4  # å›ºå®šè§’åº¦
            encoded_qc.add(RY, qreg[i], paras=[angle])

        return encoded_qc

    def _add_entanglement_layer(self, qc: QuantumCircuit, qreg: QuantumRegister):
        """
        æ·»åŠ çº ç¼ å±‚ - åŸºäºsingle_agentçš„æ–¹æ³•
        """
        # ç¯å½¢çº ç¼ ï¼šæ¯ä¸ªé‡å­æ¯”ç‰¹ä¸ä¸‹ä¸€ä¸ªé‡å­æ¯”ç‰¹çº ç¼ 
        for i in range(len(qreg) - 1):
            qc.add(CNOT, qreg[i+1], qreg[i])

        # æœ€åä¸€ä¸ªä¸ç¬¬ä¸€ä¸ªçº ç¼ ï¼Œå½¢æˆç¯
        if len(qreg) > 1:
            qc.add(CNOT, qreg[0], qreg[len(qreg)-1])

    def _execute_single_quantum_circuit(self, qc: QuantumCircuit) -> Dict[str, int]:
        """
        æ‰§è¡Œå•ä¸ªé‡å­çº¿è·¯ - single_agentçš„æ­£ç¡®æ–¹å¼
        å…³é”®ï¼šåªè°ƒç”¨ä¸€æ¬¡backend.apply()ï¼
        """
        # æ·»åŠ æµ‹é‡é—¨
        qreg = qc.qreg
        creg = qc.creg

        for i in range(len(qreg)):
            qc.add(MEASURE, qreg[i], creg[i])

        try:
            # ç¡®ä¿åç«¯å¯ç”¨
            if self.backend is None:
                self._initialize_backend()

            if self.backend is None:
                logger.warning("é‡å­åç«¯ä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                return self._generate_fallback_results(len(qreg))

            # å…³é”®ï¼šåªæ‰§è¡Œä¸€æ¬¡ï¼
            logger.info(f"ğŸ”¬ æ‰§è¡Œå•ä¸ªé‡å­çº¿è·¯: {len(qreg)}é‡å­æ¯”ç‰¹, {self.shots}æ¬¡æµ‹é‡")
            self.backend.apply(qc)
            results = self.backend.run(self.shots)

            # éªŒè¯ç»“æœ
            if not results or not isinstance(results, dict):
                logger.warning("é‡å­æ‰§è¡Œç»“æœæ— æ•ˆï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                return self._generate_fallback_results(len(qreg))

            logger.info(f"âœ… é‡å­çº¿è·¯æ‰§è¡ŒæˆåŠŸï¼Œè·å¾— {len(results)} ä¸ªæµ‹é‡ç»“æœ")
            return results

        except Exception as e:
            logger.error(f"âŒ é‡å­çº¿è·¯æ‰§è¡Œå¤±è´¥: {e}")
            logger.info("ä½¿ç”¨æ¨¡æ‹Ÿç»“æœæ›¿ä»£")
            return self._generate_fallback_results(len(qreg))



    def _generate_fallback_results(self, n_qubits: int) -> Dict[str, int]:
        """ç”Ÿæˆåå¤‡çš„æ¨¡æ‹Ÿé‡å­æµ‹é‡ç»“æœ"""
        logger.info("ç”Ÿæˆæ¨¡æ‹Ÿé‡å­æµ‹é‡ç»“æœ")

        import random
        results = {}

        # ç”Ÿæˆä¸€äº›éšæœºçš„æµ‹é‡ç»“æœ
        for _ in range(min(self.shots, 100)):  # é™åˆ¶ç»“æœæ•°é‡
            # ç”Ÿæˆéšæœºæ¯”ç‰¹ä¸²
            bit_string = ''.join(random.choice(['0', '1']) for _ in range(n_qubits))
            if bit_string in results:
                results[bit_string] += 1
            else:
                results[bit_string] = 1

        logger.info(f"ç”Ÿæˆäº† {len(results)} ä¸ªæ¨¡æ‹Ÿæµ‹é‡ç»“æœ")
        return results
    
    def _extract_quantum_features(self, tavily_data: Dict[str, Any]) -> List[float]:
        """ä»Tavilyæ•°æ®ä¸­æå–é‡å­ç‰¹å¾"""
        features = []
        
        # ç‰¹å¾1: æŠ¥å‘Šé•¿åº¦ï¼ˆæ ‡å‡†åŒ–ï¼‰
        report_length = len(tavily_data.get('report', ''))
        features.append(min(report_length / 10000.0, 1.0))
        
        # ç‰¹å¾2: æ•°æ®æºæ•°é‡
        references_count = len(tavily_data.get('references', []))
        features.append(min(references_count / 20.0, 1.0))
        
        # ç‰¹å¾3: è´¢åŠ¡æ•°æ®ä¸°å¯Œåº¦
        financial_data = tavily_data.get('financial_data', {})
        financial_richness = len(str(financial_data)) / 1000.0
        features.append(min(financial_richness, 1.0))
        
        # ç‰¹å¾4: æ–°é—»æ•°æ®æ´»è·ƒåº¦
        news_data = tavily_data.get('news_data', {})
        news_activity = len(str(news_data)) / 1000.0
        features.append(min(news_activity, 1.0))
        
        # è½¬æ¢ä¸ºè§’åº¦ç¼–ç  [0, 2Ï€]
        features = [f * 2 * np.pi for f in features]
        
        return features
    

    


    def _clear_backend(self):
        """æ¸…ç†é‡å­åç«¯çŠ¶æ€"""
        try:
            # å°è¯•æ¸…ç†å½“å‰åç«¯
            if hasattr(self.backend, 'clear') and callable(self.backend.clear):
                self.backend.clear()
                logger.debug("é‡å­åç«¯çŠ¶æ€å·²æ¸…ç†")
            elif hasattr(self.backend, 'reset') and callable(self.backend.reset):
                self.backend.reset()
                logger.debug("é‡å­åç«¯å·²é‡ç½®")
            else:
                # é‡æ–°åˆå§‹åŒ–åç«¯
                self._initialize_backend()
                logger.debug("é‡å­åç«¯å·²é‡æ–°åˆå§‹åŒ–")
        except Exception as e:
            logger.warning(f"æ¸…ç†é‡å­åç«¯æ—¶å‡ºé”™: {e}")
            # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œå¼ºåˆ¶é‡æ–°åˆå§‹åŒ–
            try:
                self.backend = None
                self._initialize_backend()
                logger.info("é‡å­åç«¯å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e2:
                logger.error(f"å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–é‡å­åç«¯å¤±è´¥: {e2}")
                # è®¾ç½®ä¸ºNoneï¼Œåç»­ä½¿ç”¨æ¨¡æ‹Ÿç»“æœ
                self.backend = None
    
    def _analyze_quantum_results(self, measurement_results: Dict[str, int], 
                               companies_quantum_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æé‡å­æµ‹é‡ç»“æœ"""
        quantum_analysis = {}
        total_shots = sum(measurement_results.values())
        
        for company_idx, company_data in enumerate(companies_quantum_data):
            company_name = company_data["name"]
            
            # è®¡ç®—è¯¥å…¬å¸çš„é‡å­ç‰¹å¾
            company_measurements = self._extract_company_measurements(
                measurement_results, company_idx, total_shots
            )
            
            quantum_features = self._compute_quantum_features(company_measurements)
            
            quantum_analysis[company_name] = {
                "quantum_features": quantum_features,
                "measurement_probability": company_measurements.get("probability", 0.0),
                "entanglement_strength": self._compute_entanglement_strength(company_measurements),
                "quantum_advantage_score": self._compute_quantum_advantage_score(quantum_features)
            }

        return quantum_analysis

    def _extract_company_measurements(self, measurement_results: Dict[str, int],
                                    company_idx: int, total_shots: int) -> Dict[str, Any]:
        """æå–ç‰¹å®šå…¬å¸çš„æµ‹é‡ç»“æœ"""
        company_results = {"probability": 0.0, "measurements": []}

        # å…¬å¸ç´¢å¼•çš„äºŒè¿›åˆ¶è¡¨ç¤º
        company_binary = format(company_idx, f'0{self.n_qubits}b')

        for bit_string, count in measurement_results.items():
            if bit_string.startswith(company_binary):
                company_results["measurements"].append((bit_string, count))
                company_results["probability"] += count / total_shots

        return company_results

    def _compute_quantum_features(self, company_measurements: Dict[str, Any]) -> List[float]:
        """è®¡ç®—é‡å­ç‰¹å¾"""
        measurements = company_measurements.get("measurements", [])

        if not measurements:
            return [0.0] * 4

        # ç‰¹å¾1: æµ‹é‡ç†µ
        probabilities = [count for _, count in measurements]
        total = sum(probabilities)
        if total > 0:
            probabilities = [p/total for p in probabilities]
            entropy = -sum(p * np.log2(p + 1e-10) for p in probabilities if p > 0)
        else:
            entropy = 0.0

        # ç‰¹å¾2: æœ€å¤§æ¦‚ç‡
        max_prob = max(probabilities) if probabilities else 0.0

        # ç‰¹å¾3: çŠ¶æ€å¤šæ ·æ€§
        num_states = len(measurements)

        # ç‰¹å¾4: å¹³å‡æ¯”ç‰¹å€¼
        avg_bit_value = 0.0
        if measurements:
            total_weight = 0
            for bit_string, count in measurements:
                bit_value = sum(int(bit) for bit in bit_string) / len(bit_string)
                avg_bit_value += bit_value * count
                total_weight += count
            if total_weight > 0:
                avg_bit_value /= total_weight

        return [entropy, max_prob, float(num_states), avg_bit_value]

    def _compute_entanglement_strength(self, company_measurements: Dict[str, Any]) -> float:
        """è®¡ç®—çº ç¼ å¼ºåº¦"""
        measurements = company_measurements.get("measurements", [])
        if len(measurements) <= 1:
            return 0.0

        probabilities = [count for _, count in measurements]
        total = sum(probabilities)
        if total == 0:
            return 0.0

        probabilities = [p/total for p in probabilities]
        max_entropy = np.log2(len(probabilities))
        actual_entropy = -sum(p * np.log2(p + 1e-10) for p in probabilities if p > 0)

        return actual_entropy / max_entropy if max_entropy > 0 else 0.0

    def _compute_quantum_advantage_score(self, quantum_features: List[float]) -> float:
        """è®¡ç®—é‡å­ä¼˜åŠ¿è¯„åˆ†"""
        if not quantum_features:
            return 0.0

        weights = [0.3, 0.3, 0.2, 0.2]
        normalized_features = []

        for i, feature in enumerate(quantum_features):
            if i == 0:  # ç†µ
                normalized_features.append(min(feature / 3.0, 1.0))
            elif i == 1:  # æœ€å¤§æ¦‚ç‡
                normalized_features.append(feature)
            elif i == 2:  # çŠ¶æ€æ•°
                normalized_features.append(min(feature / 8.0, 1.0))
            else:  # å¹³å‡æ¯”ç‰¹å€¼
                normalized_features.append(feature)

        score = sum(w * f for w, f in zip(weights, normalized_features))
        return min(score, 1.0)

    async def _generate_enhanced_reports(self, tavily_data: Dict[str, Any],
                                       quantum_results: Dict[str, Any],
                                       websocket_manager, job_id) -> Dict[str, Any]:
        """ç”Ÿæˆé‡å­å¢å¼ºçš„æŠ¥å‘Š"""
        if websocket_manager and job_id:
            await websocket_manager.send_status_update(
                job_id, status="processing",
                message="ğŸ§  Generating quantum-enhanced reports with DeepSeek..."
            )

        enhanced_reports = {}

        for company_name, tavily_report in tavily_data.items():
            quantum_meta = quantum_results.get(company_name, {})
            
            # ä»åŸå§‹å…¬å¸æ•°æ®ä¸­è·å–åŸºæœ¬ä¿¡æ¯
            company_info = next((c for c in self.original_companies if c["name"] == company_name), {})

            base_report = tavily_report.get("report", "")

            # å¦‚æœåŸºç¡€æŠ¥å‘Šä¸ºç©ºï¼Œç”ŸæˆåŸºæœ¬æ¡†æ¶
            if not base_report.strip():
                base_report = f"""# {company_name} å…¬å¸åˆ†ææŠ¥å‘Š

## å…¬å¸æ¦‚å†µ
*åŸºäºå¯è·å¾—çš„å…¬å¼€ä¿¡æ¯è¿›è¡Œåˆ†æ*

## ä¸šåŠ¡åˆ†æ
*è¯¦ç»†ä¸šåŠ¡ä¿¡æ¯æ”¶é›†ä¸­*

## è´¢åŠ¡çŠ¶å†µ
*è´¢åŠ¡æ•°æ®åˆ†æä¸­*

## å¸‚åœºåœ°ä½
*å¸‚åœºå®šä½åˆ†æä¸­*
"""

            # è°ƒç”¨DeepSeekç”Ÿæˆæœ€ç»ˆç»Ÿä¸€æŠ¥å‘Š
            final_report = await self._generate_unified_report_with_quantum_insights(
                company_name, base_report, quantum_meta, tavily_report
            )

            enhanced_report = {
                "company_name": company_name,
                "company_industry": company_info.get("industry", ""),
                "company_location": company_info.get("hq_location", ""),
                "company_url": company_info.get("company_url", ""),
                "tavily_report": base_report,
                "quantum_enhanced_analysis": final_report,
                "analysis_metadata": {
                    "company_basic_info": {
                        "name": company_name,
                        "industry": company_info.get("industry", ""),
                        "hq_location": company_info.get("hq_location", ""),
                        "company_url": company_info.get("company_url", "")
                    },
                    "tavily_data": {
                        "company_data": tavily_report.get("company_data", {}),
                        "financial_data": tavily_report.get("financial_data", {}),
                        "industry_data": tavily_report.get("industry_data", {}),
                        "news_data": tavily_report.get("news_data", {}),
                        "references": tavily_report.get("references", [])
                    },
                    "quantum_metadata": {
                        "quantum_features": quantum_meta.get("quantum_features", []),
                        "quantum_advantage_score": quantum_meta.get("quantum_advantage_score", 0.0),
                        "entanglement_strength": quantum_meta.get("entanglement_strength", 0.0),
                        "measurement_probability": quantum_meta.get("measurement_probability", 0.0),
                        "processing_timestamp": datetime.now().isoformat(),
                        "quantum_backend": "wuyue_simulator",
                        "shots_used": self.shots,
                        "quantum_layers": self.n_layers,
                        "total_qubits": self.total_qubits
                    }
                }
            }

            enhanced_reports[company_name] = enhanced_report

        return enhanced_reports

    async def _generate_unified_report_with_quantum_insights(self, company_name: str, base_report: str, 
                                                           quantum_meta: Dict[str, Any], 
                                                           tavily_data: Dict[str, Any]) -> str:
        """ä½¿ç”¨DeepSeekç”Ÿæˆèåˆé‡å­æ´å¯Ÿçš„ä¸“ä¸šé‡‘èæŠ¥å‘Š"""
        
        # æå–è¯¦ç»†çš„é‡å­åˆ†ææ•°æ®
        quantum_features = quantum_meta.get("quantum_features", [])
        quantum_advantage_score = quantum_meta.get("quantum_advantage_score", 0.0)
        entanglement_strength = quantum_meta.get("entanglement_strength", 0.0)
        measurement_probability = quantum_meta.get("measurement_probability", 0.0)
        
        # æ„å»ºè¯¦ç»†çš„é‡å­åˆ†æè§£é‡Š
        quantum_analysis_detail = self._build_quantum_analysis_context(
            quantum_features, quantum_advantage_score, entanglement_strength, measurement_probability
        )
        
        # è·å–å‚è€ƒæ•°æ®æºä¿¡æ¯
        references = tavily_data.get("references", [])
        data_sources = f"åŸºäº{len(references)}ä¸ªæƒå¨æ•°æ®æº" if references else "åŸºäºå…¬å¼€ä¿¡æ¯"
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„é‡‘èåˆ†æå¸ˆå’Œé‡åŒ–æŠ•èµ„ä¸“å®¶ã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œä¸º{company_name}ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„å…¬å¸ç ”ç©¶æŠ¥å‘Šã€‚

ã€åŸºç¡€ç ”ç©¶æŠ¥å‘Šã€‘
{base_report}

ã€é‡å­å¹¶è¡Œåˆ†ææ ¸å¿ƒæ´å¯Ÿã€‘
æœ¬åˆ†æé‡‡ç”¨äº†å…ˆè¿›çš„é‡å­å¹¶è¡Œè®¡ç®—æŠ€æœ¯ï¼ŒåŒæ—¶å¤„ç†å¤šå®¶å…¬å¸æ•°æ®ä»¥è·å¾—æ›´å‡†ç¡®çš„ç›¸å¯¹æ¯”è¾ƒç»“æœã€‚ä»¥ä¸‹æ˜¯å…³é”®é‡å­åˆ†ææŒ‡æ ‡ï¼š

1. **å¸‚åœºå·®å¼‚åŒ–æŒ‡æ•°**: {quantum_advantage_score:.4f}
   - èŒƒå›´ï¼š0-1ï¼Œå½“å‰å€¼ï¼š{quantum_advantage_score:.4f}
   - è§£è¯»ï¼š{'é«˜åº¦å·®å¼‚åŒ–ä¼ä¸šï¼Œå…·å¤‡ç‹¬ç‰¹ç«äº‰ä¼˜åŠ¿' if quantum_advantage_score > 0.7 else 'é€‚åº¦å·®å¼‚åŒ–ï¼Œæœ‰ä¸€å®šç‰¹è‰²' if quantum_advantage_score > 0.4 else 'åŒè´¨åŒ–ç¨‹åº¦è¾ƒé«˜ï¼Œéœ€è¦å¯»æ‰¾å·®å¼‚åŒ–è·¯å¾„'}
   - æŠ•èµ„æ„ä¹‰ï¼šå·®å¼‚åŒ–ç¨‹åº¦ç›´æ¥å½±å“ä¼ä¸šçš„å®šä»·æƒå’Œç›ˆåˆ©èƒ½åŠ›

2. **è¡Œä¸šå…³è”åº¦ç³»æ•°**: {entanglement_strength:.4f}
   - èŒƒå›´ï¼š0-1ï¼Œå½“å‰å€¼ï¼š{entanglement_strength:.4f}
   - è§£è¯»ï¼š{'é«˜åº¦ä¾èµ–è¡Œä¸šå‘¨æœŸï¼Œç³»ç»Ÿæ€§é£é™©è¾ƒé«˜' if entanglement_strength > 0.7 else 'ä¸­ç­‰è¡Œä¸šæ•æ„Ÿåº¦ï¼Œæœ‰ä¸€å®šç‹¬ç«‹æ€§' if entanglement_strength > 0.4 else 'ç›¸å¯¹ç‹¬ç«‹äºè¡Œä¸šè¶‹åŠ¿ï¼Œä¸ªè‚¡ç‰¹å¾æ˜æ˜¾'}
   - æŠ•èµ„æ„ä¹‰ï¼šå†³å®šäº†å®è§‚ç»æµå’Œè¡Œä¸šæ”¿ç­–å¯¹å…¬å¸çš„å½±å“ç¨‹åº¦

3. **å¸‚åœºæƒé‡æŒ‡æ•°**: {measurement_probability:.4f}
   - è§£è¯»ï¼š{'è¡Œä¸šæ ¸å¿ƒä¼ä¸šï¼Œå…·æœ‰é‡è¦å¸‚åœºåœ°ä½' if measurement_probability > 0.15 else 'é‡è¦å‚ä¸è€…ï¼Œæœ‰ä¸€å®šå½±å“åŠ›' if measurement_probability > 0.1 else 'ä¸€èˆ¬å‚ä¸è€…ï¼Œå…³æ³¨æˆé•¿æ½œåŠ›'}
   - æŠ•èµ„æ„ä¹‰ï¼šåæ˜ äº†å…¬å¸åœ¨åŒç±»ä¼ä¸šä¸­çš„ç›¸å¯¹é‡è¦æ€§å’Œå¸‚åœºå½±å“åŠ›

4. **é‡å­ç‰¹å¾å‘é‡**: {quantum_features}
   - è¿™äº›æ•°å€¼åæ˜ äº†å…¬å¸åœ¨å¤šç»´åº¦ç‰¹å¾ç©ºé—´ä¸­çš„ä½ç½®
   - é€šè¿‡é‡å­å åŠ æ€åˆ†æå¾—å‡ºï¼Œæä¾›äº†ä¼ ç»Ÿåˆ†ææ— æ³•è·å¾—çš„æ·±å±‚æ´å¯Ÿ

{quantum_analysis_detail}

ã€æ•°æ®æ¥æºã€‘
{data_sources}ï¼Œé€šè¿‡é‡å­å¹¶è¡Œå¤„ç†æŠ€æœ¯è¿›è¡Œæ·±åº¦åˆ†æ

ã€æŠ¥å‘Šè¦æ±‚ã€‘
è¯·å°†ä»¥ä¸Šä¿¡æ¯æ•´åˆæˆä¸€ä»½ä¸“ä¸šçš„é‡‘èç ”ç©¶æŠ¥å‘Šï¼Œç‰¹åˆ«æ³¨æ„ï¼š

1. **é‡å­åˆ†æçš„å•†ä¸šä»·å€¼è½¬åŒ–**ï¼šå°†é‡å­è®¡ç®—ç»“æœè½¬åŒ–ä¸ºå…·ä½“çš„å•†ä¸šæ´å¯Ÿå’ŒæŠ•èµ„å»ºè®®
2. **é£é™©è¯„ä¼°**ï¼šåŸºäºè¡Œä¸šå…³è”åº¦å’Œå¸‚åœºåœ°ä½è¯„ä¼°æŠ•èµ„é£é™©
3. **æŠ•èµ„ç­–ç•¥**ï¼šæ ¹æ®å·®å¼‚åŒ–æŒ‡æ•°å’Œå¸‚åœºæƒé‡åˆ¶å®šæŠ•èµ„ç­–ç•¥
4. **ä¸“ä¸šæœ¯è¯­**ï¼šä½¿ç”¨æ ‡å‡†çš„é‡‘èåˆ†æå’ŒæŠ•èµ„ç ”ç©¶æœ¯è¯­
5. **ç»“æ„åŒ–åˆ†æ**ï¼šä¿æŒé€»è¾‘æ¸…æ™°çš„Markdownæ ¼å¼

æŠ¥å‘Šç»“æ„åº”åŒ…å«ï¼š
- æ‰§è¡Œæ‘˜è¦ï¼ˆå«æŠ•èµ„è¯„çº§ï¼‰
- å…¬å¸æ¦‚å†µä¸ä¸šåŠ¡åˆ†æ  
- è´¢åŠ¡çŠ¶å†µåˆ†æ
- å¸‚åœºåœ°ä½ä¸ç«äº‰ä¼˜åŠ¿ï¼ˆé‡ç‚¹èå…¥é‡å­åˆ†ææ´å¯Ÿï¼‰
- é£é™©å› ç´ è¯„ä¼°
- æŠ•èµ„å»ºè®®ä¸ç›®æ ‡ä»·ä½
- é™„å½•ï¼šé‡å­åˆ†ææŠ€æœ¯è¯´æ˜

è¯·ç¡®ä¿é‡å­åˆ†æçš„ç‹¬ç‰¹ä»·å€¼åœ¨æŠ¥å‘Šä¸­å¾—åˆ°å……åˆ†ä½“ç°ã€‚"""

        try:
            # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
            if not hasattr(self, 'deepseek_client'):
                from openai import OpenAI
                api_key = os.getenv("DEEPSEEK_API_KEY")
                if not api_key:
                    raise ValueError("DEEPSEEK_API_KEY not found. DeepSeek API is required for quantum analysis.")
                
                self.deepseek_client = OpenAI(
                    api_key=api_key,
                    base_url="https://api.deepseek.com"
                )

            response = self.deepseek_client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": """ä½ æ˜¯ä¸€ä½é¡¶çº§çš„é‡‘èåˆ†æå¸ˆå’Œé‡åŒ–æŠ•èµ„ä¸“å®¶ï¼Œæ‹¥æœ‰20å¹´çš„æŠ•èµ„ç ”ç©¶ç»éªŒã€‚
ä½ æ“…é•¿å°†å¤æ‚çš„é‡åŒ–åˆ†æç»“æœè½¬åŒ–ä¸ºæ¸…æ™°çš„æŠ•èµ„æ´å¯Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ©ç”¨å…ˆè¿›è®¡ç®—æŠ€æœ¯è¿›è¡Œä¼ä¸šåˆ†ææ–¹é¢æœ‰ç‹¬åˆ°è§è§£ã€‚
ä½ çš„æŠ¥å‘Šä»¥ä¸“ä¸šæ€§ã€å‡†ç¡®æ€§å’Œå®ç”¨æ€§è‘—ç§°ï¼Œæ·±å—æœºæ„æŠ•èµ„è€…ä¿¡èµ–ã€‚"""
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.2,  # é™ä½æ¸©åº¦ç¡®ä¿ä¸“ä¸šæ€§
                max_tokens=8192,  # DeepSeekæœ€å¤§æ”¯æŒ8192 tokens
                top_p=0.9
            )
            
            unified_report = response.choices[0].message.content.strip()
            logger.info(f"âœ… DeepSeekæˆåŠŸç”Ÿæˆ{company_name}é‡å­å¢å¼ºæŠ¥å‘Š ({len(unified_report)}å­—ç¬¦)")
            return unified_report
            
        except Exception as e:
            logger.error(f"âŒ DeepSeekæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            raise Exception(f"DeepSeek APIè°ƒç”¨å¤±è´¥: {str(e)}")

    def _build_quantum_analysis_context(self, quantum_features: List[float],
                                      quantum_advantage_score: float,
                                      entanglement_strength: float,
                                      measurement_probability: float) -> str:
        """æ„å»ºé‡å­åˆ†æçš„è¯¦ç»†ä¸Šä¸‹æ–‡è¯´æ˜"""

        context = f"""
ã€é‡å­åˆ†ææŠ€æœ¯èƒŒæ™¯ã€‘
æœ¬åˆ†æé‡‡ç”¨äº†åŸºäºwuyueé‡å­æ¡†æ¶çš„å¹¶è¡Œè®¡ç®—æŠ€æœ¯ï¼Œé€šè¿‡ä»¥ä¸‹æ­¥éª¤è·å¾—æ´å¯Ÿï¼š

1. **é‡å­ç¼–ç é˜¶æ®µ**ï¼šå°†å…¬å¸çš„å¤šç»´ç‰¹å¾æ•°æ®ç¼–ç åˆ°{self.total_qubits}ä¸ªé‡å­æ¯”ç‰¹çš„å åŠ æ€ä¸­
2. **å¹¶è¡Œå¤„ç†é˜¶æ®µ**ï¼šåˆ©ç”¨é‡å­å åŠ æ€åŒæ—¶åˆ†æå¤šå®¶å…¬å¸ï¼Œå‘ç°éšè—çš„å…³è”æ¨¡å¼
3. **é‡å­æµ‹é‡é˜¶æ®µ**ï¼šé€šè¿‡{self.shots}æ¬¡é‡å­æµ‹é‡è·å¾—ç»Ÿè®¡ç»“æœ
4. **ç‰¹å¾æå–é˜¶æ®µ**ï¼šä»é‡å­æ€åç¼©ç»“æœä¸­æå–{len(quantum_features)}ç»´ç‰¹å¾å‘é‡

ã€é‡å­ä¼˜åŠ¿è¯´æ˜ã€‘
ç›¸æ¯”ä¼ ç»Ÿä¸²è¡Œåˆ†æï¼Œé‡å­å¹¶è¡Œåˆ†æå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
- **çœŸå¹¶è¡Œæ€§**ï¼šåˆ©ç”¨é‡å­å åŠ æ€å®ç°çœŸæ­£çš„åŒæ—¶è®¡ç®—ï¼Œè€Œéæ—¶é—´ç‰‡è½®è½¬
- **å…³è”å‘ç°**ï¼šé€šè¿‡é‡å­çº ç¼ è‡ªåŠ¨æ•è·å…¬å¸é—´çš„éšå«å…³è”å…³ç³»
- **ç‰¹å¾å¢å¼º**ï¼šé‡å­æµ‹é‡æä¾›ä¼ ç»Ÿæ–¹æ³•æ— æ³•è·å¾—çš„éçº¿æ€§ç‰¹å¾ç»„åˆ
- **ç›¸å¯¹æ¯”è¾ƒ**ï¼šåœ¨åŒä¸€é‡å­ç³»ç»Ÿä¸­å¤„ç†å¤šå®¶å…¬å¸ï¼Œè·å¾—æ›´å‡†ç¡®çš„ç›¸å¯¹è¯„ä¼°

ã€å…³é”®æŒ‡æ ‡è§£è¯»ã€‘
- å¸‚åœºå·®å¼‚åŒ–æŒ‡æ•°åæ˜ äº†å…¬å¸åœ¨é‡å­ç‰¹å¾ç©ºé—´ä¸­çš„ç‹¬ç‰¹æ€§
- è¡Œä¸šå…³è”åº¦ç³»æ•°æ¥æºäºé‡å­çº ç¼ å¼ºåº¦ï¼Œæ­ç¤ºäº†å…¬å¸ä¸è¡Œä¸šçš„æ·±å±‚å…³è”
- å¸‚åœºæƒé‡æŒ‡æ•°åŸºäºé‡å­æµ‹é‡æ¦‚ç‡ï¼Œä½“ç°äº†å…¬å¸çš„ç›¸å¯¹é‡è¦æ€§

è¿™äº›æŒ‡æ ‡çš„ç»„åˆä¸ºæŠ•èµ„å†³ç­–æä¾›äº†å…¨æ–°çš„é‡åŒ–ç»´åº¦ã€‚"""

        return context

    async def _save_to_knowledge_base(self, enhanced_reports: Dict[str, Any],
                                    original_companies: List[Dict[str, str]]) -> Dict[str, Any]:
        """ä¿å­˜åˆ°çŸ¥è¯†åº“"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ä¿å­˜å•ä¸ªå…¬å¸æŠ¥å‘Š
        for company_name, report in enhanced_reports.items():
            # ä»åŸå§‹å…¬å¸æ•°æ®ä¸­è·å–è¡Œä¸šå’Œä½ç½®ä¿¡æ¯
            company_info = next((c for c in original_companies if c["name"] == company_name), {})
            industry = company_info.get("industry", "æœªçŸ¥è¡Œä¸š").replace("/", "-").replace("\\", "-")
            location = company_info.get("hq_location", "æœªçŸ¥ä½ç½®").replace("/", "-").replace("\\", "-")
            
            # æ–°çš„å‘½åæ ¼å¼: å…¬å¸å_è¡Œä¸š_ä½ç½®_quantum_enhanced_æ—¶é—´æˆ³.json
            safe_company_name = company_name.replace("/", "-").replace("\\", "-")
            filename = f"{safe_company_name}_{industry}_{location}_quantum_enhanced_{timestamp}.json"
            filepath = os.path.join(self.company_reports_dir, filename)

            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            logger.info(f"ğŸ“ {company_name} é‡å­å¢å¼ºæŠ¥å‘Šå·²ä¿å­˜: {filepath}")

        # ä¿å­˜æ‰¹é‡åˆ†ææ‘˜è¦
        batch_summary = {
            "batch_id": f"quantum_batch_{timestamp}",
            "timestamp": timestamp,
            "analysis_type": "quantum_parallel_enhanced",
            "total_companies": len(enhanced_reports),
            "successful_count": len(enhanced_reports),
            "companies_analyzed": list(enhanced_reports.keys()),
            "quantum_parameters": {
                "total_qubits": self.total_qubits,
                "quantum_layers": self.n_layers,
                "measurement_shots": self.shots,
                "max_companies": self.max_companies
            },
            "quantum_statistics": {
                "avg_quantum_advantage": np.mean([
                    report["analysis_metadata"]["quantum_metadata"]["quantum_advantage_score"]
                    for report in enhanced_reports.values()
                ]),
                "avg_entanglement_strength": np.mean([
                    report["analysis_metadata"]["quantum_metadata"]["entanglement_strength"]
                    for report in enhanced_reports.values()
                ])
            },
            "input_companies": original_companies,
            "reports_location": self.company_reports_dir
        }

        batch_filename = f"quantum_batch_analysis_{timestamp}.json"
        batch_filepath = os.path.join(self.batch_results_dir, batch_filename)

        with open(batch_filepath, 'w', encoding='utf-8') as f:
            json.dump(batch_summary, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ“Š é‡å­æ‰¹é‡åˆ†ææ‘˜è¦å·²ä¿å­˜: {batch_filepath}")
        return batch_summary

    # ä¸ºäº†ä¿æŒå‘åå…¼å®¹æ€§ï¼Œæ·»åŠ æ–¹æ³•åˆ«å
    async def quantum_parallel_analyze(self, companies: List[Dict[str, str]],
                                     websocket_manager=None, job_id=None) -> Dict[str, Any]:
        """
        é‡å­å¹¶è¡Œåˆ†ææ–¹æ³• - è¿™æ˜¯ä¸»è¦çš„å…¬å…±æ¥å£
        ä¸ºäº†ä¿æŒä¸application.pyä¸­è°ƒç”¨çš„å…¼å®¹æ€§
        """
        return await self.process_companies_quantum_parallel(
            companies, websocket_manager, job_id
        )










